{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strands Agents SDK와 Claude 4 인터리브드 씽킹을 사용한 도구로서의 에이전트\n",
    "\n",
    "이 노트북은 Strands Agents SDK와 Claude 4의 **인터리브드 씽킹** 기능을 사용하여 전문 에이전트와 함께 지능적인 워크플로우를 조율하는 방법을 보여줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인터리브드 씽킹 이해하기\n",
    "\n",
    "### 인터리브드 씽킹이란?\n",
    "\n",
    "인터리브드 씽킹은 Claude 4 모델의 새로운 기능으로 모델이 다음을 수행할 수 있게 합니다:\n",
    "\n",
    "1. **도구 호출 사이에 사고**: 다음 단계를 결정하기 전에 결과를 처리하고 추론합니다\n",
    "2. **추론과 함께 여러 도구 연결**: 정교한 다단계 결정을 내립니다\n",
    "3. **전략을 동적으로 적응**: 중간 결과를 기반으로 접근 방식을 변경합니다\n",
    "\n",
    "### 작동 방식\n",
    "\n",
    "인터리브드 씽킹이 있는 것과 없는 것으로 구현된 에이전트의 이벤트 루프 사이에는 많은 유사점이 있습니다:\n",
    "```\n",
    "쿼리 → LLM이 사고 중 -> LLM이 도구 호출 결정 -> 이벤트 루프가 도구 호출 -> 출력이 LLM으로 다시 전송 -> [ LLM이 더 이상 도구를 호출할 필요가 없을 때까지 계속 - 최종 답변 렌더링 ]\n",
    "```\n",
    "\n",
    "인터리브드 씽킹에서 주목할 주요 차이점은 이벤트 루프가 \"결정\"이 아닌 LLM의 \"사고\"에 따라 작동한다는 것입니다. 위 루프의 두 번째 링크인 \"사고\"를 주목하세요. 전통적인 이벤트 루프에서는 사고가 숨겨져 있습니다. LLM이 도구 호출 결정을 렌더링하거나 최종 답변을 생성할 때까지 기다려야 합니다.\n",
    "\n",
    "인터리브드 씽킹의 경우, LLM이 여전히 두 번째 단계인 \"LLM이 사고 중\"에 있는 동안 이벤트 루프로 \"사고\"를 \"누출\"하고, 이벤트 루프는 LLM이 그것에 대해 \"생각\"하자마자 도구를 실행하도록 구성됩니다. 이는 LLM이 사고를 마칠 때까지 실제로 첫 번째 \"결정\"에서 최종 답변을 가지고 있다는 것을 의미합니다.\n",
    "\n",
    "\n",
    "### 인터리브드 씽킹 활성화\n",
    "\n",
    "Strands와 Bedrock에서 이 기능을 활성화하려면:\n",
    "- `temperature=1` 설정 (씽킹이 활성화될 때 필수)\n",
    "- 베타 헤더 추가: `\"anthropic_beta\": [\"interleaved-thinking-2025-05-14\"]`\n",
    "- 추론 예산 구성: `\"reasoning_config\": {\"type\": \"enabled\", \"budget_tokens\": 3000}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 설정 및 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from strands import Agent, tool\n",
    "from strands.models import BedrockModel\n",
    "from strands.models import bedrock\n",
    "\n",
    "bedrock.DEFAULT_BEDROCK_MODEL_ID = \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전문 에이전트를 도구로 정의\n",
    "\n",
    "먼저 Strands `@tool` 데코레이터를 사용하여 네 개의 전문 에이전트를 생성하겠습니다:\n",
    "- **연구원**: 사실적 정보 수집\n",
    "- **데이터 분석가**: 정보 처리 및 분석\n",
    "- **팩트 체커**: 정보 정확성 검증\n",
    "- **보고서 작성자**: 세련된 최종 문서 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strands @tool 데코레이터를 사용하여 도구로 구현된 전문 에이전트\n",
    "@tool\n",
    "def researcher(query: str) -> str:\n",
    "    \"\"\"\n",
    "    사실적 정보를 수집하는 연구 전문가.\n",
    "    \n",
    "    Args:\n",
    "        query: 조사할 연구 질문 또는 주제\n",
    "        \n",
    "    Returns:\n",
    "        연구 결과 및 출처\n",
    "    \"\"\"\n",
    "    # 집중된 연구 에이전트 생성\n",
    "    # 참고: 각 호출은 새로운 에이전트 인스턴스를 생성합니다 (상태 없음)\n",
    "    research_agent = Agent(\n",
    "        model=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",  # 선택사항: 모델 ID 지정\n",
    "        system_prompt=\"당신은 연구 전문가입니다. 사실적 정보를 수집하고 가능할 때 출처를 인용하세요. 응답을 200단어 이하로 유지하세요.\",\n",
    "        callback_handler=None  # 도구 에이전트에는 스트리밍 없음\n",
    "    )\n",
    "    \n",
    "    # 연구 작업 실행\n",
    "    result = research_agent(f\"연구: {query}\")\n",
    "    return str(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def data_analyst(data: str) -> str:\n",
    "    \"\"\"\n",
    "    정보를 처리하고 분석하는 데이터 분석가.\n",
    "    \n",
    "    Args:\n",
    "        data: 분석할 원시 데이터 또는 연구 결과\n",
    "        \n",
    "    Returns:\n",
    "        통찰력과 패턴이 포함된 분석\n",
    "    \"\"\"\n",
    "    # 분석가 에이전트는 통찰력 추출에 집중\n",
    "    analysis_agent = Agent(\n",
    "        model=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "        system_prompt=\"당신은 데이터 분석가입니다. 핵심 통찰력을 추출하고, 패턴을 식별하며, 분석적 결론을 제공하세요. 실행 가능한 통찰력에 집중하세요.\",\n",
    "        callback_handler=None\n",
    "    )\n",
    "    \n",
    "    # 제공된 데이터 분석\n",
    "    result = analysis_agent(f\"이 데이터를 분석하고 통찰력을 제공하세요: {data}\")\n",
    "    return str(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def fact_checker(information: str) -> str:\n",
    "    \"\"\"\n",
    "    정보 정확성을 검증하는 팩트 체커.\n",
    "    \n",
    "    Args:\n",
    "        information: 검증할 주장 또는 데이터\n",
    "        \n",
    "    Returns:\n",
    "        정확성 평가가 포함된 팩트 체크 결과\n",
    "    \"\"\"\n",
    "    # 검증을 위한 팩트 체킹 에이전트\n",
    "    fact_check_agent = Agent(\n",
    "        model=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "        system_prompt=\"당신은 팩트 체커입니다. 주장을 검증하고, 신뢰성을 평가하며, 신뢰도 수준을 제공하세요. 의심스러운 진술을 식별하세요.\",\n",
    "        callback_handler=None\n",
    "    )\n",
    "    \n",
    "    # 정보 검증\n",
    "    result = fact_check_agent(f\"이 정보를 팩트 체크하세요: {information}\")\n",
    "    return str(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def report_writer(analysis: str) -> str:\n",
    "    \"\"\"\n",
    "    세련된 최종 문서를 생성하는 보고서 작성자.\n",
    "    \n",
    "    Args:\n",
    "        analysis: 분석된 데이터와 통찰력\n",
    "        \n",
    "    Returns:\n",
    "        형식화된 최종 보고서\n",
    "    \"\"\"\n",
    "    # 전문적인 출력을 위한 작성자 에이전트\n",
    "    writer_agent = Agent(\n",
    "        system_prompt=\"당신은 전문 보고서 작성자입니다. 경영진 요약과 실행 가능한 권장사항이 포함된 명확하고 잘 구조화된 보고서를 작성하세요.\",\n",
    "        callback_handler=None\n",
    "    )\n",
    "    \n",
    "    # 보고서 생성\n",
    "    result = writer_agent(f\"다음을 기반으로 전문 보고서를 작성하세요: {analysis}\")\n",
    "    return str(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인터리브드 씽킹을 사용한 Claude 4 오케스트레이터\n",
    "\n",
    "이제 인터리브드 씽킹을 사용하여 전문 에이전트를 지능적으로 조정하는 Claude 4 에이전트인 오케스트레이터를 생성합니다.\n",
    "\n",
    "### 오케스트레이터 작동 방식:\n",
    "\n",
    "1. 사용자로부터 고수준 작업을 받습니다\n",
    "2. 어떤 정보가 필요한지 **생각**합니다\n",
    "3. 초기 데이터를 수집하기 위해 연구원 도구를 호출합니다\n",
    "4. 연구 결과와 어떤 분석이 필요한지 **생각**합니다\n",
    "5. 결과를 처리하기 위해 데이터 분석가를 호출합니다\n",
    "6. 정확성과 검증 필요성에 대해 **생각**합니다\n",
    "7. 필요한 경우 팩트 체커를 호출할 수 있습니다\n",
    "8. 결과를 어떻게 제시할지 **생각**합니다\n",
    "9. 최종 출력을 위해 보고서 작성자를 호출합니다\n",
    "10. 응답하기 전에 전체 워크플로우에 대해 **반성**합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strands를 사용한 인터리브드 씽킹이 포함된 Claude 4 오케스트레이터\n",
    "class StrandsInterlevedWorkflowOrchestrator:\n",
    "    def __init__(self):\n",
    "        # 지능적인 워크플로우 조정을 위한 오케스트레이터 시스템 프롬프트 정의\n",
    "        self.system_prompt = \"\"\"당신은 전문 에이전트에 접근할 수 있는 지능적인 워크플로우 오케스트레이터입니다.\n",
    "\n",
    "        당신의 역할은 다음 전문 에이전트를 사용하여 워크플로우를 지능적으로 조정하는 것입니다:\n",
    "        - researcher: 모든 주제에 대한 사실적 정보 수집\n",
    "        - data_analyst: 데이터 분석 및 통찰력 추출\n",
    "        - fact_checker: 정보의 정확성 검증\n",
    "        - report_writer: 세련된 최종 보고서 생성\n",
    "\n",
    "        \"\"\"\n",
    "    \n",
    "    def run_workflow(self, task: str, enable_interleaved_thinking: bool = True) -> str:\n",
    "        \"\"\"주어진 작업에 대한 지능적인 워크플로우를 실행합니다.\n",
    "        \n",
    "        Args:\n",
    "            task: 완료할 작업\n",
    "            enable_interleaved_thinking: 인터리브드 씽킹 활성화 여부 (기본값: True)\n",
    "        \n",
    "        오케스트레이터는 다음을 수행합니다:\n",
    "        1. 작업 요구사항 이해\n",
    "        2. 최선의 접근 방식에 대해 생각\n",
    "        3. 전문 에이전트 조정\n",
    "        4. 단계 간 결과에 대해 반성\n",
    "        5. 포괄적인 출력 생성\n",
    "        \"\"\"\n",
    "        thinking_mode = \"인터리브드 씽킹과 함께\" if enable_interleaved_thinking else \"인터리브드 씽킹 없이\"\n",
    "        print(f\"\\n{thinking_mode} 지능적인 워크플로우 시작: {task}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Bedrock을 통해 인터리브드 씽킹이 있거나 없는 Claude 4 구성\n",
    "        if enable_interleaved_thinking:\n",
    "            claude4_model = BedrockModel(\n",
    "                model_id=\"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "                max_tokens=4096,\n",
    "                temperature=1,  # 씽킹이 활성화될 때 1이어야 함\n",
    "                additional_request_fields={\n",
    "                    # 인터리브드 씽킹 베타 기능 활성화\n",
    "                    \"anthropic_beta\": [\"interleaved-thinking-2025-05-14\"],\n",
    "                    # 추론 매개변수 구성\n",
    "                    \"reasoning_config\": {\n",
    "                        \"type\": \"enabled\",  # 씽킹 켜기\n",
    "                        \"budget_tokens\": 3000  # 씽킹 토큰 예산\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            claude4_model = BedrockModel(\n",
    "                model_id=\"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "                max_tokens=4096,\n",
    "                temperature=1\n",
    "            )\n",
    "        \n",
    "        # Claude 4와 전문 도구로 오케스트레이터 에이전트 생성\n",
    "        orchestrator = Agent(\n",
    "            model=claude4_model,\n",
    "            system_prompt=self.system_prompt,\n",
    "            tools=[researcher, data_analyst, fact_checker, report_writer]\n",
    "        )\n",
    "        \n",
    "        prompt = f\"\"\"지능적인 워크플로우 조정을 사용하여 이 작업을 완료하세요: {task}\n",
    "\n",
    "        지침:\n",
    "        1. 이 작업을 수행하기 위해 어떤 정보가 필요한지 신중히 생각하세요\n",
    "        2. 전문 에이전트를 전략적으로 사용하세요 - 각각 고유한 강점이 있습니다\n",
    "        3. 각 도구 사용 후 결과를 반성하고 접근 방식을 조정하세요\n",
    "        4. 포괄적인 결과를 위해 필요에 따라 여러 에이전트를 조정하세요\n",
    "        5. 적절할 때 팩트 체킹을 통해 정확성을 보장하세요\n",
    "        6. 모든 측면을 다루는 포괄적인 최종 응답을 제공하세요\n",
    "        \n",
    "        기억하세요: 도구 호출 사이의 사고는 더 나은 결정을 내리는 데 도움이 됩니다.\n",
    "        이를 사용하여 계획하고, 결과를 평가하고, 전략을 조정하세요.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            result = orchestrator(prompt)\n",
    "            return str(result)\n",
    "        except Exception as e:\n",
    "            return f\"워크플로우 실패: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데모 실행\n",
    "\n",
    "오케스트레이터가 작동하는 모습을 보겠습니다! 사고하고 사고하는 동안 도구를 호출하는 방식을 관찰하세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오케스트레이터 생성\n",
    "print(\"Strands Agents SDK: Claude 4 인터리브드 씽킹 워크플로우 데모\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    orchestrator = StrandsInterlevedWorkflowOrchestrator()\n",
    "    print(\"✅ 오케스트레이터가 성공적으로 초기화되었습니다!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 오케스트레이터 초기화 실패: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 케이스로 워크플로우 실행\n",
    "test_case = \"원격 근무가 생산성에 미치는 영향을 분석하고 전략적 권장사항을 제공하세요\"\n",
    "\n",
    "print(f\"📋 작업: {test_case}\\n\")\n",
    "\n",
    "try:\n",
    "    result = orchestrator.run_workflow(test_case)\n",
    "    \n",
    "    print(f\"\\n📊 워크플로우 결과:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"❌ 워크플로우 실행 실패: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인터리브드 씽킹 없이 시도\n",
    "\n",
    "오케스트레이터를 호출하고 인터리브드 씽킹을 비활성화하여 실험해보세요. 출력의 차이를 관찰하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 인터리브드 씽킹 없이 같은 작업을 시도해보겠습니다\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"🔄 이제 인터리브드 씽킹 없이 같은 작업을 실행합니다\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    result_without_thinking = orchestrator.run_workflow(test_case, enable_interleaved_thinking=False)\n",
    "    \n",
    "    print(f\"\\n📊 워크플로우 결과 (인터리브드 씽킹 없음):\")\n",
    "    print(\"=\" * 70)\n",
    "    print(result_without_thinking)\n",
    "except Exception as e:\n",
    "    print(f\"❌ 워크플로우 실행 실패: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
